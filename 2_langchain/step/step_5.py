import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
# API KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼
from dotenv import load_dotenv
# API KEY ì •ë³´ë¡œë“œ
load_dotenv()
st.title("â˜‘ï¸ë‚˜ë§Œì˜ ì±—GPT í…ŒìŠ¤íŠ¸ğŸ˜‚")

# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ
if "messages" not in st.session_state:
    # ëŒ€íšŒê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.
    st.session_state["messages"] = []


# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)

# ì„¸ë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
def add_messages(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))

# ì²´ì¸ ìƒì„±
def create_chain():
    # prompt | llm | output+parser
    # í”„ë¡¬í”„íŠ¸
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤"),
            ("user", "#Question:\n{question}"),
        ]
    )
    # ëª¨ë¸
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
    # ì¶œë ¥íŒŒì„œ
    output_parser = StrOutputParser()
    # ì²´ì¸ìƒì„±
    chain = prompt | llm | output_parser
    return chain

#ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
print_messages()
# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")
# ë§Œì•½ ì‚¬ìš©ì ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´
if user_input:
    # ì‚¬ìš©ì ì…ë ¥
    st.chat_message("user").write(user_input)
    # AIì˜ ë‹µë³€
    chain = create_chain()
    ai_answer = chain.invoke({"question": user_input})
    st.chat_message("assistant").write(ai_answer)
    # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥
    add_messages("user", user_input)
    add_messages("assistant", ai_answer)
