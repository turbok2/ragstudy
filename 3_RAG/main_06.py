# pdfë¥¼ í•œë²ˆ ì½ì–´ì„œ ì„ë² ë”©í–ˆìœ¼ë©´, 
# ë‹¤ì‹œ ë™ì¼í•œ íŒŒì¼ì´ ì„ íƒë˜ë©´ ì„ë² ë”© ì•ˆí•˜ê³  ê¸°ì¡´ì˜ ê²ƒì„ ì‚¬ìš©í•˜ê²Œ ìˆ˜ì •
# FAISS íŒŒì¼ì— ì €ì¥ : ì¬ì‹¤í–‰í•´ë„ ë‚¨ì•„ìˆìŒ.(ê°œì¸ìš©ìœ¼ë¡œ ì í•©)
import streamlit as st
import os
import pdfplumber
from langchain_core.messages.chat import ChatMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.schema import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.chains import RetrievalQAWithSourcesChain
import hashlib
import pickle
# API KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼
from dotenv import load_dotenv
# API KEY ì •ë³´ë¡œë“œ
load_dotenv()
# ì›¹í˜ì´ì§€ íƒ­ ê¾¸ë¯¸ê¸°
st.set_page_config(
    page_title="ì§ˆì˜ì‘ë‹µì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded",
)
st.title("â¤ï¸PDF ì½ê³  ì§ˆë¬¸í•˜ê¸°_V3â¤ï¸")
# íŒŒì¼ í•´ì‹œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def get_file_hash(uploaded_file):
    file_contents = uploaded_file.read()
    file_hash = hashlib.md5(file_contents).hexdigest()
    uploaded_file.seek(0)  # íŒŒì¼ í¬ì¸í„°ë¥¼ ì²˜ìŒìœ¼ë¡œ ëŒë ¤ì„œ ì´í›„ ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ í•¨
    return file_hash

def load_data(uploaded_file):
    # íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆì„ ë•Œë§Œ ì²˜ë¦¬
    documents = []
    if uploaded_file is not None:
        # ë©”ëª¨ë¦¬ ë‚´ PDF íŒŒì¼ ì½ê¸°
        with pdfplumber.open(uploaded_file) as pdf:
            for page_number, page in enumerate(pdf.pages, start=1):
                text = page.extract_text()
                if text:  # í˜ì´ì§€ì— í…ìŠ¤íŠ¸ê°€ ìˆì„ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
                    # ê° í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ì™€ í•´ë‹¹ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ Documentë¡œ ì €ì¥
                    documents.append(
                        Document(
                            page_content=text,
                            metadata={
                                "source": uploaded_file.name,
                                "page": page_number  # í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
                            }
                        )
                    )
    return documents
# FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def load_or_create_faiss_index(file_hash, documents, embeddings, embeddings_dir="embeddings"):
    faiss_file = os.path.join(embeddings_dir, f"{file_hash}.faiss")
    meta_file = os.path.join(embeddings_dir, f"{file_hash}_meta.pkl")

    if not os.path.exists(embeddings_dir):
        os.makedirs(embeddings_dir)

    if os.path.exists(faiss_file) and os.path.exists(meta_file):
        vectorstore = FAISS.load_local(faiss_file, embeddings, allow_dangerous_deserialization=True)
        with open(meta_file, "rb") as f:
            vectorstore.docstore._dict = pickle.load(f)
        st.success("ê¸°ì¡´ ì„ë² ë”©ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    else:
        text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        split_documents = text_splitter.split_documents(documents)
        vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)
        vectorstore.save_local(faiss_file)
        with open(meta_file, "wb") as f:
            pickle.dump(vectorstore.docstore._dict, f)
        st.success("ìƒˆë¡œìš´ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    
    return vectorstore
# ì‚¬ì´ë“œ ë°” ìƒì„±
with st.sidebar:
    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    if clear_btn:
        # st.write("ë²„íŠ¼ì´ ëˆŒë ¸ìŠµë‹ˆë‹¤.")
        st.session_state["messages"] = []
    reset_btn = st.button("ì„ë² ë”© ì´ˆê¸°í™”")
    if reset_btn:
        st.session_state["file_hash"] = None  # ì´ˆê¸°í™” ì‹œ íŒŒì¼ í•´ì‹œë„ ì´ˆê¸°í™”
    # CSV íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type=["pdf"])

# ì„ë² ë”©ì´ ì´ë¯¸ ìƒì„±ëœ ê²½ìš° ì´ë¥¼ í™œìš©í•˜ë„ë¡ ìˆ˜ì •
if uploaded_file is not None:
    file_hash = get_file_hash(uploaded_file)
    
    if "file_hash" not in st.session_state or st.session_state["file_hash"] != file_hash:
        st.session_state["file_hash"] = file_hash
        documents = load_data(uploaded_file)
        embeddings = OpenAIEmbeddings()
        vectorstore = load_or_create_faiss_index(file_hash, documents, embeddings)
        st.session_state["retriever"] = vectorstore.as_retriever(search_kwargs={"k": 2})
else:
    st.session_state["file_hash"] = None
    st.session_state["retriever"] = None
        
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)

# ì„¸ë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
def add_messages(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))

def create_chain():
    system_template = """Use the following pieces of context to answer the users question shortly.
    Given the following summaries of a long document and a question, create a final answer with references ("SOURCES"), use "SOURCES" in capital letters regardless of the number of sources.
    If you don't know the answer, just say that "I don't know", don't try to make up an answer.
    ----------------
    {summaries}
    If you don't know the answer, just say that you don't know. 
    You MUST answer in Korean and in Markdown format:"""

    messages = [
        SystemMessagePromptTemplate.from_template(system_template),
        HumanMessagePromptTemplate.from_template("{question}"),
    ]

    prompt = ChatPromptTemplate.from_messages(messages)

    chain_type_kwargs = {"prompt": prompt}

    llm = ChatOpenAI(model_name="gpt-4o", temperature=0, max_tokens=2000)

    chain = RetrievalQAWithSourcesChain.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=st.session_state["retriever"],
        return_source_documents=True,
        chain_type_kwargs=chain_type_kwargs,
    )
    return chain

print_messages()
# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")
# ë§Œì•½ ì‚¬ìš©ì ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´
if user_input:
    # ì‚¬ìš©ì ì…ë ¥
    st.chat_message("user").write(user_input)
    if uploaded_file:
        # AIì˜ ë‹µë³€
        chain = create_chain()
        response = chain.invoke({"question": user_input})
        # ë‹µë³€ ì¶”ì¶œ
        ai_answer = response.get("answer", "")
        
        # ì¶œì²˜ ë¬¸ì„œ ì¶”ì¶œ (ì¶œì²˜ ì •ë³´ì— í˜ì´ì§€ ì •ë³´ê°€ í¬í•¨ë¨)
        source_documents = response.get("source_documents", [])
        sources_info = ""
        
        for doc in source_documents:
            page_number = doc.metadata.get("page", "N/A")  # í˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œ
            sources_info += f"(ì¶œì²˜: {doc.metadata.get('source')} - {page_number} í˜ì´ì§€)\n"
        
        # ë‹µë³€ê³¼ ì¶œì²˜ ì •ë³´ë¥¼ í•¨ê»˜ í‘œì‹œ
        with st.chat_message("assistant"):
            container = st.empty()
            if ai_answer=="I don't know.":
                container.markdown(f"{ai_answer}")  # ë‹µë³€ ì¶œë ¥
            else:
                container.markdown(f"{ai_answer}\n\n{sources_info}")  # ë‹µë³€ê³¼ ì¶œì²˜ ì¶œë ¥
        # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•œë‹¤.
        add_messages("user", user_input)
        add_messages("assistant", f"{ai_answer}\n\n{sources_info}")    
            
    else:
        msg = 'PDFíŒŒì¼ì„ ë¨¼ì € ì„ íƒí•´ì£¼ì„¸ìš”.'
        st.chat_message("assistant") .write(msg)   
        # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•œë‹¤.
        add_messages("user", user_input)
        add_messages("assistant", msg)    
    
# ì§ˆë¬¸
# ì‹ ê²½ë§ì²˜ë¦¬ ê¸°ë°˜ ì¸ê³µì§€ëŠ¥ ì „ìš©ì¹©ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.    (20í˜ì´ì§€)
# ì§€ëŠ¥í˜• ê°œì¸ ë§ì¶¤ ì„œë¹„ìŠ¤ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ìš”ì†Œê¸°ìˆ ì€?     (22í˜ì´ì§€)

# ì†Œë…„ì´ ì†Œë…€ë¥¼ ë§Œë‚œ ê³³ì€?
